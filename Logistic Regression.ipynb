{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDRegressor as sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # The constructor for Logistic Regression\n",
    "    def __init__ ( self,shape, loss = 'mse', lr = 0.001, iters = 1000,initialization = None ):\n",
    "        \n",
    "        # The list to hold the exponential value of w*x, for further easy calculating gradient\n",
    "        self.exponentialList = []\n",
    "         # The list to hold the sigmoid value of w*x, for further easy calculating gradient\n",
    "        self.sigmoidList = []\n",
    "        # The list to hold the predicted value\n",
    "        self.predictionList = []\n",
    "        self.predictionValList = []\n",
    "        self.sigmoidValList = []\n",
    "        self.exponentialValList = []\n",
    "        # Initialization of w vector\n",
    "        self.w = initialization\n",
    "        # The learning rate, default is 0.0001\n",
    "        self.lr = lr\n",
    "        # The iterations, default is 1000\n",
    "        self.iters = iters\n",
    "        \n",
    "        '''\n",
    "        The loss function for prediction, default setting is mean square error,\n",
    "        or another choice is binary cross entropy.\n",
    "        The loss function for mean squared error is :\n",
    "            loss = (y_actual - y_pred)**2 \n",
    "            y_pred = total_reads * probability\n",
    "            y_actual = methylated_counts\n",
    "        \n",
    "        The loss function for binary cross entropy is :\n",
    "            loss = -(methylated_counts * np.log(probability) + unmethylated_counts*np.log(1-probability))\n",
    "            Methylated_counts and unmethylated_counts are read from outside file.\n",
    "        '''\n",
    "        if loss != 'mse' and loss != 'binary_crossentropy':\n",
    "            raise ValueError(\"Incorrect Loss Function. Argument should be mse or binary_crossentropy\")\n",
    "        self.loss = loss\n",
    "        \n",
    "    # The sigmoid Funtion implementation\n",
    "    # During the calculation, we would store the result for future reference\n",
    "    def sigmoid(self,data,train):\n",
    "\n",
    "        if train:\n",
    "            # If the dimension does not match\n",
    "            if len(self.exponentialList) != data.shape[0]:\n",
    "                self.exponentialList = [0]*data.shape[0]\n",
    "                self.sigmoidList = [0]*data.shape[0]\n",
    "                self.predictionList = [0]*data.shape[0] \n",
    "\n",
    "            for i,d in enumerate(data):\n",
    "                exponential = np.exp(-np.matmul(d,self.w))\n",
    "                sigmoid = 1/(1+exponential)\n",
    "            \n",
    "                # Store the value for future calculating the gradients\n",
    "                self.exponentialList[i] = exponential\n",
    "                self.sigmoidList[i] = sigmoid\n",
    "        else:\n",
    "            # If the dimension does not match\n",
    "            if len(self.exponentialValList) != data.shape[0]:\n",
    "                self.exponentialValList = [0]*data.shape[0]\n",
    "                self.sigmoidValList = [0]*data.shape[0]\n",
    "                self.predictionValList = [0]*data.shape[0] \n",
    "            for i,d in enumerate(data):\n",
    "                exponential = np.exp(-np.matmul(d,self.w))\n",
    "                sigmoid = 1/(1+exponential)\n",
    "            \n",
    "                # Store the value for future calculating the gradients\n",
    "                self.exponentialValList[i] = exponential\n",
    "                self.sigmoidValList[i] = sigmoid\n",
    "    \n",
    "    # Prediction function for Logistic Regression\n",
    "    def predict (self,data, reads,train):\n",
    "        # Calculate the sigmoid function for each DNA sequence\n",
    "        self.sigmoid(data,train)\n",
    "        for i in range(len(data)):\n",
    "            if train:\n",
    "                # Calculate the expected value for this binomial distribution\n",
    "                self.predictionList[i] = reads[i] * self.sigmoidList[i]\n",
    "            else:\n",
    "                self.predictionValList[i] = reads[i] * self.sigmoidValList[i]\n",
    "    \n",
    "    # Gradient Calculation during training\n",
    "    def gradient(self,data, reads, methy):\n",
    "        \n",
    "        '''\n",
    "        data: kmers counts\n",
    "        reads: total reads for one DNA sequence\n",
    "        methy: methylated counts for that DNA sequence\n",
    "        '''\n",
    "        \n",
    "        #Initialize the gradient\n",
    "        gradient = np.zeros(self.w.shape)\n",
    "        \n",
    "        if self.loss == 'mse':\n",
    "            \n",
    "            for i,d in enumerate(data):\n",
    "                gradient += 2*(methy[i]-reads[i] * self.sigmoidList[i])*(-1)*reads[i]*\\\n",
    "                (self.sigmoidList[i]**2)*self.exponentialList[i]*d\n",
    "            #Take the normalization of gradient\n",
    "            gradient = gradient/sum(reads)\n",
    "            return gradient\n",
    "        # If the loss function is binomial crossentropy\n",
    "        else:\n",
    "            for i,d in enumerate(data):\n",
    "                gradient += reads[i]*self.sigmoidList[i]*self.exponentialList[i]*(-1)*d + \\\n",
    "                (reads[i] - methy[i])*d\n",
    "            #Take the normalization of gradient\n",
    "            gradient = gradient/sum(reads)\n",
    "            return gradient\n",
    "        \n",
    "    # The loss function calculation   \n",
    "    def lossCal (self, data, reads, methy,train=True):\n",
    "        # predict the data and then store the results\n",
    "        self.predict(data,reads,train)\n",
    "        \n",
    "        loss = 0\n",
    "        if self.loss == 'mse':\n",
    "            for i in range(len(data)):\n",
    "                loss += (self.predictionList[i] - methy[i])**2\n",
    "            return loss/len(data)\n",
    "        else:\n",
    "            for i in range(len(data)):\n",
    "                loss += -(methy[i]*np.log(self.sigmoidList[i]) + (reads[i] - methy[i])*np.log(1-self.sigmoidList[i]))\n",
    "            return loss/len(data)\n",
    "        \n",
    "    def fit(self,data, reads, methy,test_data,test_reads,test_methy):\n",
    "        #initialize lists for storing variable\n",
    "        self.exponentialList = [0]*data.shape[0]\n",
    "        self.sigmoidList = [0]*data.shape[0]\n",
    "        self.predictionList = [0]*data.shape[0] \n",
    "        \n",
    "        currLoss = [self.lossCal(data,reads,methy)]\n",
    "        valLoss = [self.lossCal(test_data,test_reads,test_methy,train=False)]\n",
    "        for i in range(self.iters):\n",
    "            # calculate gradient\n",
    "            step = self.gradient(data,reads,methy)\n",
    "            self.w -= self.lr * step\n",
    "            # Rerun Logistic Regression, calculate the loss, update the lists to store the data\n",
    "            newLoss = self.lossCal(data,reads,methy)\n",
    "            newValLoss = self.lossCal(test_data,test_reads,test_methy,train=False)\n",
    "            if abs(newLoss - currLoss[-1]) < 0.00001:\n",
    "                print(\"The iterator stops at \", i, \" steps\")\n",
    "                break\n",
    "            currLoss.append(newLoss)\n",
    "            valLoss.append(newValLoss)\n",
    "            if i % 20 == 0:\n",
    "                print(\"Training epoch: \", i)\n",
    "                print(\"\\tTraining Loss: \",newLoss)\n",
    "                print('\\tValidation Loss: ',newValLoss)\n",
    "        return currLoss,valLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58959, 2080) (58959,) (58959,) (58959,)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../CNN_Keras/SimulationCNN/Kmers6_counts_600bp.csv')\n",
    "train_reads = pd.read_csv('../data/Mouse_DMRs_counts_total.csv',header = None)\n",
    "train_methys = pd.read_csv('../data/Mouse_DMRs_counts_methylated.csv',header = None)\n",
    "train_methy_level = pd.read_csv('../data/Mouse_DMRs_methylation_level.csv',header = None)\n",
    "cell_type = 5\n",
    "data = train_data.as_matrix()\n",
    "level = train_methy_level.as_matrix()[:,cell_type]\n",
    "reads = train_reads.as_matrix()[:,cell_type]\n",
    "methy = train_methys.as_matrix()[:,cell_type]\n",
    "print(data.shape,level.shape,reads.shape,methy.shape)\n",
    "data_train = data[:48000]\n",
    "level_train = level[:48000]\n",
    "reads_train = reads[:48000]\n",
    "methy_train = methy[:48000]\n",
    "data_test = data[48000:]\n",
    "level_test = level[48000:]\n",
    "reads_test = reads[48000:]\n",
    "methy_test = methy[48000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=200, penalty='l1', power_t=0.25,\n",
       "       random_state=None, shuffle=True, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sgd(loss = 'squared_loss',penalty = 'l1',n_iter=200)\n",
    "clf.fit(data_train,level_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2648154729228096"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(data_train,level_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(train_data.shape,lr=0.005,loss = 'binary_crossentropy',iters = 500,initialization = clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch:  0\n",
      "\tTraining Loss:  53.991161265743415\n",
      "\tValidation Loss:  55.3310142646287\n",
      "Training epoch:  20\n",
      "\tTraining Loss:  47.429377663558064\n",
      "\tValidation Loss:  48.649384784005704\n",
      "Training epoch:  40\n",
      "\tTraining Loss:  47.06048124511136\n",
      "\tValidation Loss:  48.32848971014192\n",
      "Training epoch:  60\n",
      "\tTraining Loss:  46.969776743882335\n",
      "\tValidation Loss:  48.31806684709823\n",
      "Training epoch:  80\n",
      "\tTraining Loss:  46.8987526583388\n",
      "\tValidation Loss:  48.33113332338361\n",
      "Training epoch:  100\n",
      "\tTraining Loss:  46.83343113462243\n",
      "\tValidation Loss:  48.34816707285665\n",
      "Training epoch:  120\n",
      "\tTraining Loss:  46.772476630601176\n",
      "\tValidation Loss:  48.366920779677805\n",
      "Training epoch:  140\n",
      "\tTraining Loss:  46.71532931701174\n",
      "\tValidation Loss:  48.386816088836014\n",
      "Training epoch:  160\n",
      "\tTraining Loss:  46.661548391831964\n",
      "\tValidation Loss:  48.40755041869552\n",
      "Training epoch:  180\n",
      "\tTraining Loss:  46.61076313748873\n",
      "\tValidation Loss:  48.42890807191841\n",
      "Training epoch:  200\n",
      "\tTraining Loss:  46.562659366126276\n",
      "\tValidation Loss:  48.45072179826983\n",
      "Training epoch:  220\n",
      "\tTraining Loss:  46.51696967789996\n",
      "\tValidation Loss:  48.47285962908058\n",
      "Training epoch:  240\n",
      "\tTraining Loss:  46.47346550799203\n",
      "\tValidation Loss:  48.49521727642236\n",
      "Training epoch:  260\n",
      "\tTraining Loss:  46.43195058134133\n",
      "\tValidation Loss:  48.51771249889844\n",
      "Training epoch:  280\n",
      "\tTraining Loss:  46.39225552378732\n",
      "\tValidation Loss:  48.5402806329221\n",
      "Training epoch:  300\n",
      "\tTraining Loss:  46.3542334239877\n",
      "\tValidation Loss:  48.5628710029115\n",
      "Training epoch:  320\n",
      "\tTraining Loss:  46.31775617627187\n",
      "\tValidation Loss:  48.585444035778956\n",
      "Training epoch:  340\n",
      "\tTraining Loss:  46.28271146459652\n",
      "\tValidation Loss:  48.60796894540467\n",
      "Training epoch:  360\n",
      "\tTraining Loss:  46.249000272641375\n",
      "\tValidation Loss:  48.630421878340464\n",
      "Training epoch:  380\n",
      "\tTraining Loss:  46.216534825712095\n",
      "\tValidation Loss:  48.65278443211766\n",
      "Training epoch:  400\n",
      "\tTraining Loss:  46.18523688707387\n",
      "\tValidation Loss:  48.67504247406637\n",
      "Training epoch:  420\n",
      "\tTraining Loss:  46.15503634531381\n",
      "\tValidation Loss:  48.69718520210148\n",
      "Training epoch:  440\n",
      "\tTraining Loss:  46.12587004079254\n",
      "\tValidation Loss:  48.71920440003006\n",
      "Training epoch:  460\n",
      "\tTraining Loss:  46.09768078864394\n",
      "\tValidation Loss:  48.741093848965384\n",
      "Training epoch:  480\n",
      "\tTraining Loss:  46.07041656348498\n",
      "\tValidation Loss:  48.76284886379102\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = lr.fit(data_train,reads_train,methy_train,data_test,reads_test,methy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59298eaeb8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.82898666904892"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(lr.sigmoidValList, level_test)\n",
    "r2_score(lr.sigmoidList,level_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
